import torch
import torch.nn as nn
import torch.onnx
from torchinfo import summary
from torchviz import make_dot
import onnx
import onnxruntime

#@ profile
def test_model(
    model, 
    inputs, 
    labels, 
    criterion=None, 
    optimizer=None, 
    onnx_file_path="model_test.onnx"
):
    """
    é€šç”¨åŒ–æ¨¡å‹æµ‹è¯•å‡½æ•°ï¼š
    1. æ¥å—ä»»æ„æ¨¡å‹å®ä¾‹åŒ–å¯¹è±¡ `model`ã€‚
    2. è‡ªå®šä¹‰è¾“å…¥ `inputs` å’Œæ ‡ç­¾ `labels`ã€‚
    3. æ”¯æŒå‰å‘ä¼ æ’­ã€åå‘ä¼ æ’­ã€æŸå¤±è®¡ç®—ã€‚
    4. å¯¼å‡º ONNX æ¨¡å‹å¹¶éªŒè¯ã€‚
    5. è¾“å‡ºæ¨¡å‹è¯¦ç»†ä¿¡æ¯ã€‚
    
    å‚æ•°ï¼š
    - model: PyTorch æ¨¡å‹å®ä¾‹åŒ–å¯¹è±¡: torch.nn.Module
    - inputs: æ¨¡å‹çš„è¾“å…¥å¼ é‡: tensor æˆ– tulple(tensor1, tensor2, ...) æˆ– list(tensor1, tensor2, ...)
    - labels: æ¨¡å‹çš„çœŸå®æ ‡ç­¾å¼ é‡ï¼ˆç”¨äºæŸå¤±è®¡ç®—ï¼‰: tensor
    - criterion: æŸå¤±å‡½æ•°å®ä¾‹åŒ–å¯¹è±¡ï¼Œé»˜è®¤ä¸º nn.MSELoss
    - optimizer: ä¼˜åŒ–å™¨å®ä¾‹åŒ–å¯¹è±¡ï¼Œé»˜è®¤ä¸º Adam
    - onnx_file_path: å¯¼å‡ºçš„ ONNX æ–‡ä»¶è·¯å¾„
    """
    # é»˜è®¤æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    if criterion is None:
        criterion = nn.MSELoss()
    if optimizer is None:
        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

    # å°†æ¨¡å‹è®¾ä¸ºè®­ç»ƒæ¨¡å¼
    model.train()
    print("\n~~~~~~~~~~~~~~~~~~~ ğŸš€ğŸš€ å¼€å§‹æµ‹è¯•ç¥ç»ç½‘ç»œæ¨¡å‹æ˜¯å¦å¯ä»¥æ­£å¸¸è®­ç»ƒ ğŸš€ğŸš€ ~~~~~~~~~~~~~~~~~~~~")
    # æ‰“å°æ¨¡å‹ç»“æ„ä¿¡æ¯
    print("\n============== æ¨¡å‹ç»“æ„ä¿¡æ¯ ==============")
    _input_data = tuple(inputs) if isinstance(inputs, (tuple, list)) else inputs
    summary(
        model,
        input_data=_input_data,
        col_names=["input_size", "output_size", "num_params"],
        depth=3,
        device="cuda" if next(model.parameters()).is_cuda else "cpu"
    )
    
    # å‰å‘ä¼ æ’­ä¸lossè®¡ç®—
    print("\n============== å‰å‘ä¼ æ’­ ==============")
    if isinstance(inputs, (tuple, list)):
        outputs = model(*inputs)
        # ä¸€è¡Œæ‰“å°æ¨¡å‹å„ä¸ªè¾“å…¥inputçš„å½¢çŠ¶
        print(f"âœ” æ¨¡å‹å„ä¸ªè¾“å…¥çš„å½¢çŠ¶ï¼š{[input.shape for input in inputs]}")

    else:
        outputs = model(inputs)
        print(f"âœ” è¾“å…¥å½¢çŠ¶ï¼š{inputs.shape}")

    if isinstance(outputs, (tuple, list)):
        # ä¸€è¡Œæ‰“å°æ¨¡å‹å„ä¸ªè¾“å‡ºoutputçš„å½¢çŠ¶
        print(f"âœ” æ¨¡å‹å„ä¸ªè¾“å‡ºçš„å½¢çŠ¶ï¼š{[output.shape for output in outputs]}")
        for i, output in enumerate(outputs):
            if labels.shape == output.shape:
                loss = criterion(output, labels)
                print(f"âœ” ç¬¬{i+1}ä¸ªæ¨¡å‹è¾“å‡ºå¯¹åº”äº†ä¸€ä¸ªlosså€¼: {loss.item()}")

    else:
        print(f"âœ” æ¨¡å‹è¾“å‡ºå½¢çŠ¶ï¼š{outputs.shape}")
        if labels.shape == outputs.shape:
            loss = criterion(outputs, labels)
            print(f"âœ” æŸå¤±å€¼ï¼š{loss.item()}")
        else: 
            print("âœ˜ æ¨¡å‹è¾“å‡ºå½¢çŠ¶ä¸æ ‡ç­¾å½¢çŠ¶ä¸åŒ¹é…ï¼Œæ— æ³•è®¡ç®—æŸå¤±å€¼")


    # åå‘ä¼ æ’­
    print("\n============== åå‘ä¼ æ’­ ==============")
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    print("âœ” åå‘ä¼ æ’­æ­£å¸¸~")

    # å¯è§†åŒ–è®¡ç®—å›¾
    print("\n============== è®¡ç®—å›¾å¯è§†åŒ– ==============")
    graph = make_dot(loss, params=dict(model.named_parameters()))
    graph.render("model_computation_graph", format="png")
    print("âœ” è®¡ç®—å›¾å·²ä¿å­˜ä¸º 'model_computation_graph.png'")

    # å¯¼å‡º ONNX æ¨¡å‹
    print("\n============== å¯¼å‡º ONNX æ¨¡å‹ ==============")
    torch.onnx.export(
        model,
        _input_data,
        onnx_file_path,
        input_names=[f"input_{i}" for i in range(len(inputs))],
        output_names=["output"],
        dynamic_axes={f"input_{i}": {0: "batch_size"} for i in range(len(inputs))},
        opset_version=11,
    )
    print(f"âœ” ONNX æ¨¡å‹å·²ä¿å­˜è‡³ {onnx_file_path}")
    print("åœ¨ https://netron.app/ ä¸ŠæŸ¥çœ‹ ONNX æ¨¡å‹ç»“æ„")

    # éªŒè¯ ONNX æ¨¡å‹
    print("\n============== éªŒè¯ ONNX æ¨¡å‹ ==============")
    onnx_model = onnx.load(onnx_file_path)
    onnx.checker.check_model(onnx_model)
    print("âœ” ONNX æ¨¡å‹éªŒè¯æˆåŠŸï¼")

    # # ä½¿ç”¨ ONNX Runtime æ¨ç†
    # print("\n============== ONNX Runtime æ¨ç† ==============")
    # ort_session = onnxruntime.InferenceSession(onnx_file_path)
    # ort_inputs = {onnx_model.graph.input[i].name: inputs[i].cpu().numpy() if isinstance(inputs, (tuple, list)) else inputs.cpu().numpy()
    #               for i in range(len(onnx_model.graph.input))}
    # ort_outs = ort_session.run(None, ort_inputs)
    # print(f"ONNX æ¨ç†è¾“å‡ºï¼š{ort_outs}")


# ç¤ºä¾‹ç”¨æ³•
if __name__ == "__main__":
    from utils.models import TeacherModel, StudentModel

    # æ¨¡å‹å®ä¾‹åŒ–
    num_classes_of_discrete = [7, 2, 2, 3]  # ç¦»æ•£ç‰¹å¾çš„ç±»åˆ«æ•°
    Teachermodel = TeacherModel(num_classes_of_discrete).cuda()
    Studentmodel = StudentModel(num_classes_of_discrete).cuda()

    # ç¤ºä¾‹è¾“å…¥æ•°æ®ï¼ˆæ¨¡æ‹Ÿæ•°æ®é›†ç¬¬1ä¸ªbatchï¼‰
    batch_size = 128
    time_steps = 150

    # x_acc è¾“å…¥ï¼Œå½¢çŠ¶ (batch_size, 2, time_steps)
    x_acc = torch.randn(batch_size, 2, time_steps).cuda()  # éšæœºç”ŸæˆåŠ é€Ÿåº¦æ›²çº¿æ•°æ®

    # x_att_continuous è¾“å…¥ï¼Œå½¢çŠ¶ (batch_size, 4)
    x_att_continuous = torch.randn(batch_size, 4).cuda()  # éšæœºç”Ÿæˆè¿ç»­ç‰¹å¾

    # x_att_discrete è¾“å…¥ï¼Œå½¢çŠ¶ (batch_size, 4)
    x_att_discrete = torch.cat([
        torch.randint(0, 7, (batch_size, 1)),  # collision overlap rate (0~6)
        torch.randint(0, 2, (batch_size, 1)),  # belt usage (0/1)
        torch.randint(0, 2, (batch_size, 1)),  # airbag usage (0/1)
        torch.randint(0, 3, (batch_size, 1))   # occupant size (0/1/2)
    ], dim=1).cuda()  # éšæœºç”Ÿæˆç¦»æ•£ç‰¹å¾

    # æ¨¡æ‹ŸçœŸå®æ ‡ç­¾
    y_HIC = torch.randn(batch_size).cuda()  # éšæœºç”Ÿæˆ HIC æ ‡ç­¾

    # æµ‹è¯•æ¨¡å‹
    test_model(Teachermodel, inputs=(x_acc, x_att_continuous, x_att_discrete), labels=y_HIC)
    #test_model(Studentmodel, inputs=(x_att_continuous, x_att_discrete), labels=y_HIC)